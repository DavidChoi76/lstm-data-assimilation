{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Metrics Switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag to signal whether it's necessary to load all of hte results\n",
    "# and calculate new performance statistics.\n",
    "# This takes several hours.\n",
    "CALCULATE_NEW_METRICS = True\n",
    "\n",
    "# If we calculate new performance statistics, they are saved in this file.\n",
    "# If we do not calculate new performance statistics, old ones are loaded\n",
    "# from this file.\n",
    "time_split_stats_filename = 'results/time_split_statistics.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The neuralhydroloy repository must exist in the current working directory.\n",
    "from neuralhydrology.evaluation import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available metrics are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NSE',\n",
       " 'MSE',\n",
       " 'RMSE',\n",
       " 'KGE',\n",
       " 'Alpha-NSE',\n",
       " 'Pearson-r',\n",
       " 'Beta-KGE',\n",
       " 'Beta-NSE',\n",
       " 'FHV',\n",
       " 'FMS',\n",
       " 'FLV',\n",
       " 'Peak-Timing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of all available performance metrics in NeuralHydrology\n",
    "metrics_list = metrics.get_available_metrics()\n",
    "print('The available metrics are:')\n",
    "metrics_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metrics that we are going to calculate are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NSE', 'KGE', 'Alpha-NSE', 'Pearson-r', 'Beta-KGE', 'Beta-NSE', 'Peak-Timing']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any metrics that you don't care about. \n",
    "# Feel free to change this.\n",
    "metrics_list.remove('MSE')\n",
    "metrics_list.remove('RMSE')\n",
    "metrics_list.remove('FHV')\n",
    "metrics_list.remove('FMS')\n",
    "metrics_list.remove('FLV')\n",
    "\n",
    "print('The metrics that we are going to calculate are:')\n",
    "metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 531 basins.\n"
     ]
    }
   ],
   "source": [
    "basin_file = '531_basin_list.txt'\n",
    "with Path(basin_file).open('r') as fp:\n",
    "    basins = sorted(basin.strip() for basin in fp if basin.strip())\n",
    "print(f\"There are {len(basins)} basins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to results from different models. \n",
    "# Note that these must match what is in the config files.\n",
    "autoregression_dir = 'runs/time_split/autoregression'\n",
    "assimilation_dir = 'runs/time_split/assimilation'\n",
    "simulation_dir = 'runs/time_split/simulation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 simulation runs.\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all simulation results.\n",
    "simulation_run_dirs = glob.glob(simulation_dir + '/simulation_*')\n",
    "for i, run_dir in enumerate(simulation_run_dirs):\n",
    "    simulation_run_dirs[i] = run_dir.split('/')[-1]\n",
    "print(f\"There are {len(simulation_run_dirs)} simulation runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 25 assimilation runs.\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all assimilation results.\n",
    "assimilation_run_dirs = glob.glob(assimilation_dir + '/**')\n",
    "for i, run_dir in enumerate(assimilation_run_dirs):\n",
    "    assimilation_run_dirs[i] = run_dir.split('/')[-1]\n",
    "print(f\"There are {len(assimilation_run_dirs)} assimilation runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 150 autoregression runs.\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all autoregression results.\n",
    "autoregression_run_dirs = glob.glob(autoregression_dir + '/**')\n",
    "for i, run_dir in enumerate(autoregression_run_dirs):\n",
    "    autoregression_run_dirs[i] = run_dir.split('/')[-1]\n",
    "print(f\"There are {len(autoregression_run_dirs)} autoregression runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Infer the \"dimensions\" of the experiment. This informatoin is available from the\n",
    "# directory names.\n",
    "lead_times = set()\n",
    "train_holdout_fractions = set()\n",
    "test_holdout_fractions = set()\n",
    "ensembles = set()\n",
    "\n",
    "\n",
    "for run in autoregression_run_dirs:\n",
    "    if 'train_holdout' in run:\n",
    "        train_holdout_fractions.add(float(run.split('train_holdout_')[-1].split('_')[0]))    \n",
    "        test_holdout_fractions.add(float(run.split('test_holdout_')[-1].split('_')[0]))    \n",
    "    lead_times.add(int(run.split('lead_')[-1].split('_')[0]))  \n",
    "    ensembles.add(int(run.split('seed_')[-1].split('_')[0]))\n",
    "\n",
    "train_holdout_fractions = sorted(list(train_holdout_fractions))\n",
    "test_holdout_fractions = sorted(list(test_holdout_fractions))\n",
    "lead_times = sorted(list(lead_times))\n",
    "ensembles = sorted(list(ensembles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes over the experiment \"dimensions\" for AR and DA.\n",
    "ar_index = pd.MultiIndex.from_product((train_holdout_fractions, test_holdout_fractions, lead_times, ensembles))\n",
    "da_index = pd.MultiIndex.from_product((train_holdout_fractions, lead_times, ensembles))\n",
    "\n",
    "# Create mappings from metric names to data frames of metric values.\n",
    "autoregression_statistics = {}\n",
    "assimilation_statistics = {}\n",
    "simulation_statistics = {}\n",
    "for metric in metrics_list:\n",
    "    autoregression_statistics[metric] = pd.DataFrame(index=basins, columns=ar_index, dtype=np.float64)\n",
    "    assimilation_statistics[metric] = pd.DataFrame(index=basins, columns=da_index, dtype=np.float64)\n",
    "    simulation_statistics[metric] = pd.DataFrame(index=basins, columns=ensembles, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Obs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The observation data set from experiments with non-zero holdout fractions are incomplete. \n",
    "# It is necessary to use the observation record with no holdout to calculate performance \n",
    "# statistics. Get this from one of the simulation directories.\n",
    "# All experiments use the same train/test split, so this time period works for everything.\n",
    "with open(Path(simulation_dir) / simulation_run_dirs[0] / 'test/model_epoch030/test_results.p', 'rb') as f:\n",
    "    data_xr = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9138c32202e64f90ba97a1e687bade2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/531 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CALCULATE_NEW_METRICS:\n",
    "    for run in simulation_run_dirs:\n",
    "        ensemble = int(run.split('seed_')[-1].split('_')[0])\n",
    "\n",
    "        with open(simulation_dir + '/' + run + '/test/model_epoch030/test_results.p', 'rb') as f:\n",
    "            run_data = pkl.load(f)\n",
    "\n",
    "        for basin in tqdm(basins):\n",
    "\n",
    "            sim = run_data[basin]['1D']['xr'].stack(datetime=['date', 'time_step'])['QObs(mm/d)_sim']\n",
    "            sim['datetime'] = sim.coords['date']# + sim.coords['time_step']\n",
    "            obs = data_xr[basin]['1D']['xr']['QObs(mm/d)_obs'].stack(datetime=['date', 'time_step'])\n",
    "            obs['datetime'] = obs.coords['date']# + obs.coords['time_step']\n",
    "            basin_metrics = metrics.calculate_metrics(obs=obs, sim=sim, metrics=metrics_list)\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                simulation_statistics[metric].loc[basin, ensemble] = basin_metrics[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.796015\n",
       "dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_statistics['NSE'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assimilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d284ea1a68f045ce8c9ce0717a186520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CALCULATE_NEW_METRICS:\n",
    "    for run in tqdm(assimilation_run_dirs):\n",
    "        hf = float(run.split('holdout_')[-1].split('_')[0])   \n",
    "        lead_time = int(run.split('lead_')[-1].split('_')[0])\n",
    "        ensemble = int(run.split('seed_')[-1].split('_')[0])\n",
    "\n",
    "        if lead_time > 1:\n",
    "            continue\n",
    "        if hf > 0:\n",
    "            continue\n",
    "\n",
    "        with open(assimilation_dir + '/' + run + '/test/model_epoch030/test_results_data_assimilation.p', 'rb') as f:\n",
    "            run_data = pkl.load(f)\n",
    "\n",
    "        for basin in basins:\n",
    "            sim = run_data[basin]['1D']['xr'].stack(datetime=['date', 'time_step'])['QObs(mm/d)_sim']\n",
    "            sim['datetime'] = sim.coords['date']# + sim.coords['time_step']\n",
    "            obs = data_xr[basin]['1D']['xr']['QObs(mm/d)_obs'].stack(datetime=['date', 'time_step'])\n",
    "            obs['datetime'] = obs.coords['date']# + obs.coords['time_step']\n",
    "            basin_metrics = metrics.calculate_metrics(obs=obs, sim=sim, metrics=metrics_list)\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                assimilation_statistics[metric].loc[basin, (hf, lead_time, ensemble)] = basin_metrics[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00  1   0    0.86165\n",
       "      2   0        NaN\n",
       "      4   0        NaN\n",
       "      8   0        NaN\n",
       "      10  0        NaN\n",
       "0.25  1   0        NaN\n",
       "      2   0        NaN\n",
       "      4   0        NaN\n",
       "      8   0        NaN\n",
       "      10  0        NaN\n",
       "0.50  1   0        NaN\n",
       "      2   0        NaN\n",
       "      4   0        NaN\n",
       "      8   0        NaN\n",
       "      10  0        NaN\n",
       "0.75  1   0        NaN\n",
       "      2   0        NaN\n",
       "      4   0        NaN\n",
       "      8   0        NaN\n",
       "      10  0        NaN\n",
       "1.00  1   0        NaN\n",
       "      2   0        NaN\n",
       "      4   0        NaN\n",
       "      8   0        NaN\n",
       "      10  0        NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assimilation_statistics['NSE'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96be595bcba4418588b807ef03ddf13c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/150 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CALCULATE_NEW_METRICS:\n",
    "    for run in tqdm(autoregression_run_dirs):\n",
    "        if 'train_holdout' not in run:\n",
    "            continue\n",
    "        train_hf = float(run.split('train_holdout_')[-1].split('_')[0])   \n",
    "        test_hf = float(run.split('test_holdout_')[-1].split('_')[0])   \n",
    "        lead_time = int(run.split('lead_')[-1].split('_')[0])\n",
    "        ensemble = int(run.split('seed_')[-1].split('_')[0])\n",
    "        \n",
    "        with open(autoregression_dir + '/' + run + '/test/model_epoch030/test_results.p', 'rb') as f:\n",
    "            run_data = pkl.load(f)\n",
    "\n",
    "        for basin in basins:\n",
    "            sim = run_data[basin]['1D']['xr'].stack(datetime=['date', 'time_step'])['QObs(mm/d)_sim']\n",
    "            sim['datetime'] = sim.coords['date']# + sim.coords['time_step']\n",
    "            obs = data_xr[basin]['1D']['xr']['QObs(mm/d)_obs'].stack(datetime=['date', 'time_step'])\n",
    "            obs['datetime'] = obs.coords['date']# + obs.coords['time_step']\n",
    "            basin_metrics = metrics.calculate_metrics(obs=obs, sim=sim, metrics=metrics_list)\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                autoregression_statistics[metric].loc[basin, (train_hf, test_hf, lead_time, ensemble)] = basin_metrics[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoregression_statistics['NSE'].median().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Precaculated Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we calculated new metrics, save these over any old ones tha tmight exist.\n",
    "if CALCULATE_NEW_METRICS:\n",
    "    with open(time_split_stats_filename, 'wb') as f:\n",
    "        pkl.dump([\n",
    "            simulation_statistics,\n",
    "            autoregression_statistics,\n",
    "            assimilation_statistics\n",
    "        ], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the saved metrics. \n",
    "# Even if we just calcuated them, might as well make sure the file works.\n",
    "with open(time_split_stats_filename, 'rb') as f:\n",
    "    simulation_statistics, autoregression_statistics, assimilation_statistics = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1\n",
    "\n",
    "Median NSE scores of AR models trained and tested with different fractions of lagged streamflow data withheld. The two subplots\n",
    "show the same results, but organized by the amount of lagged streamflow data withheld during training vs. during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By test holdout\n",
    "lead_time = 1\n",
    "metric = 'NSE'\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "for hf in test_holdout_fractions:\n",
    "    plotdata = autoregression_statistics[metric].median().loc[hf] \n",
    "    plotdata = plotdata.loc[(slice(None), lead_time, 0)]\n",
    "    axes[0].plot(test_holdout_fractions, plotdata.values, label=hf)\n",
    "axes[1].plot([0,1], [simulation_statistics[metric].values[0], simulation_statistics[metric].values[0]], 'k--', label='Sim')\n",
    "axes[0].set_xlabel('test holdout fraction')\n",
    "axes[0].legend(title=\"train holdout fraction\", prop={'size': 6})\n",
    "axes[0].set_ylabel(metric)\n",
    "axes[0].grid()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "for hf in train_holdout_fractions:\n",
    "    plotdata = autoregression_statistics[metric].median().loc[(slice(None), hf)] \n",
    "    plotdata = plotdata.loc[(slice(None), lead_time, 0)]\n",
    "    print(plotdata.values)\n",
    "    asdf\n",
    "    axes[1].plot(test_holdout_fractions, plotdata.values, label=hf)\n",
    "axes[1].plot([0,1], [simulation_statistics[metric].values[0], simulation_statistics[metric].values[0]], 'k--', label='Sim')\n",
    "axes[1].set_xlabel('train holdout fraction')\n",
    "axes[1].legend(title=\"test holdout fraction\", prop={'size': 6})\n",
    "axes[1].set_ylabel(metric)\n",
    "axes[1].grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_name = f\"./results/plots/figure1_ar_holdout.png\"\n",
    "plt.savefig(plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2\n",
    "\n",
    "Comparison of per-basin NSEs with an observation lag of one day and no missing streamflow input data: (left) autoregression\n",
    "trained with no holdout data vs. variational assimilation and (right) autoregression with vs. without 50% of lagged streamflow data withheld\n",
    "during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'NSE'\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "xdata = autoregression_statistics[metric].median().loc[(0.0, 0.0, 1, 0)]\n",
    "ydata = assimilation_statistics[metric][(0.0, 1, 0)]\n",
    "axes[0].scatter(xdata, ydata)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--')\n",
    "axes[0].set_xlabel('AR 0.0 Holdout')\n",
    "axes[0].set_ylabel('Variational Assimilation')\n",
    "axes[0].set_title('NSE (1-day Lag, 0% Missing Data)')\n",
    "axes[0].grid()\n",
    "\n",
    "xdata = autoregression_statistics[metric].median().loc[(0.0, 0.0, 1, 0)]\n",
    "ydata = autoregression_statistics[metric].median().loc[(0.5, 0.0, 1, 0)]\n",
    "axes[1].scatter(xdata, ydata)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "axes[1].set_xlabel('AR 0.0 Holdout')\n",
    "axes[1].set_ylabel('AR 0.5 Holdout')\n",
    "axes[1].set_title('NSE (1-day Lag, 0% Missing Test Data)')\n",
    "axes[1].grid()\n",
    "\n",
    "plot_file_name = f\"./plots/figure2_scatterplot.png\"\n",
    "plt.savefig(plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 & Appendix G\n",
    "\n",
    "Median NSE over 531 basins of four models (simulation, autoregression trained with and without holdout data, and data assimilation) as a function of lag time in days and fraction of missing lagged streamflow data in the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_list:\n",
    "\n",
    "    ar_plotdata_hf = pd.DataFrame(index=lead_times, columns=train_holdout_fractions)\n",
    "    for lt in lead_times:\n",
    "        for hf in train_holdout_fractions:\n",
    "            ar_plotdata_hf.loc[lt, hf] = autoregression_statistics[metric].median().loc[(hf, hf, lt, 0)]\n",
    "\n",
    "    ar_plotdata_0 = pd.DataFrame(index=lead_times, columns=train_holdout_fractions)\n",
    "    for lt in lead_times:\n",
    "        for hf in train_holdout_fractions:\n",
    "            ar_plotdata_0.loc[lt, hf] = autoregression_statistics[metric].median().loc[(0.0, hf, lt, 0)]\n",
    "\n",
    "    da_plotdata = pd.DataFrame(index=lead_times, columns=train_holdout_fractions)\n",
    "    for lt in lead_times:\n",
    "        for hf in train_holdout_fractions:\n",
    "            da_plotdata.loc[lt, hf] = assimilation_statistics[metric].median().loc[(hf, lt, 0)]\n",
    "    \n",
    "    ymin = np.min([simulation_statistics[metric].values[0], ar_plotdata_hf.min().min(), ar_plotdata_0.min().min(), da_plotdata.min().min()]) * 0.99\n",
    "    ymax = np.max([simulation_statistics[metric].values[0], ar_plotdata_hf.max().max(), ar_plotdata_0.max().max(), da_plotdata.max().max()]) * 1.01\n",
    "\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(8, 7))\n",
    "    for i, lead_time in enumerate(lead_times):\n",
    "        axes.flatten()[i].plot(simulation_statistics[metric].median(), label='Sim')\n",
    "        axes.flatten()[i].plot(ar_plotdata_hf.loc[lead_time], label='AR w/ holdout')\n",
    "        axes.flatten()[i].plot(ar_plotdata_0.loc[lead_time], label='AR w/o holdout')\n",
    "        axes.flatten()[i].plot(da_plotdata.loc[lead_time], label='DA')\n",
    "        if i == 5: axes.flatten()[i].legend()\n",
    "        axes.flatten()[i].set_xlabel('fraction of missing data')\n",
    "        axes.flatten()[i].set_ylim([ymin, ymax])\n",
    "        axes.flatten()[i].set_title(f'{lead_time} Days')\n",
    "        \n",
    "    plot_file_name = f\"./plots/{metric}_lead_times_plots.png\"\n",
    "    plt.savefig(plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2\n",
    "\n",
    "Median NSE scores of AR models trained and tested with different fractions of lagged streamflow data withheld. The two subplots\n",
    "show the same results, but organized by the amount of lagged streamflow data withheld during training vs. during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow this to work with ensembles instead of just pulling the 0th member.\n",
    "models_list = ['Simulation', 'AR 0.0 holdout', 'AR 0.5 holdout', 'Assimilation']\n",
    "table_df = pd.DataFrame(index=metrics_list, columns=models_list)\n",
    "for metric in metrics_list:\n",
    "    table_df.loc[metric, 'Simulation'] = simulation_statistics[metric].median().values[0]\n",
    "    table_df.loc[metric, 'AR 0.0 holdout'] = autoregression_statistics[metric].median().loc[(0.0, 0.0, 1, 0)]\n",
    "    table_df.loc[metric, 'AR 0.5 holdout'] = autoregression_statistics[metric].median().loc[(0.5, 0.0, 1, 0)]\n",
    "    table_df.loc[metric, 'Assimilation'] = assimilation_statistics[metric].median().loc[(0.0, 1, 0)]\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in latex format.\n",
    "table_filename = 'results/tables/zero_lag_zero_holdout_metrics_table.txt''\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "with open(table_filename, 'wt') as f:\n",
    "    f.write(table_df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
