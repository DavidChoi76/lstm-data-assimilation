{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Metrics Switch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boolean flag to signal whether it's necessary to load all of hte results\n",
    "# and calculate new performance statistics.\n",
    "# This takes several hours.\n",
    "CALCULATE_NEW_METRICS = True\n",
    "\n",
    "# If we calculate new performance statistics, they are saved in this file.\n",
    "# If we do not calculate new performance statistics, old ones are loaded\n",
    "# from this file.\n",
    "stats_filename = 'results/pub_statistics.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# The neuralhydroloy repository must exist in the current working directory.\n",
    "from neuralhydrology.evaluation import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The available metrics are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NSE',\n",
       " 'MSE',\n",
       " 'RMSE',\n",
       " 'KGE',\n",
       " 'Alpha-NSE',\n",
       " 'Pearson-r',\n",
       " 'Beta-KGE',\n",
       " 'Beta-NSE',\n",
       " 'FHV',\n",
       " 'FMS',\n",
       " 'FLV',\n",
       " 'Peak-Timing']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Returns a list of all available performance metrics in NeuralHydrology\n",
    "metrics_list = metrics.get_available_metrics()\n",
    "print('The available metrics are:')\n",
    "metrics_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The metrics that we are going to calculate are:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NSE', 'KGE', 'Alpha-NSE', 'Pearson-r', 'Beta-KGE', 'Beta-NSE', 'Peak-Timing']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove any metrics that you don't care about. \n",
    "# Feel free to change this.\n",
    "metrics_list.remove('MSE')\n",
    "metrics_list.remove('RMSE')\n",
    "metrics_list.remove('FHV')\n",
    "metrics_list.remove('FMS')\n",
    "metrics_list.remove('FLV')\n",
    "\n",
    "print('The metrics that we are going to calculate are:')\n",
    "metrics_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 531 basins.\n"
     ]
    }
   ],
   "source": [
    "basin_file = '531_basin_list.txt'\n",
    "with Path(basin_file).open('r') as fp:\n",
    "    basins = sorted(basin.strip() for basin in fp if basin.strip())\n",
    "print(f\"There are {len(basins)} basins.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to results from different models. \n",
    "# Note that these must match what is in the config files.\n",
    "autoregression_dir = 'runs/pub/autoregression'\n",
    "assimilation_dir = 'runs/pub/assimilation'\n",
    "simulation_dir = 'runs/pub/simulation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 simulation runs.\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all simulation results.\n",
    "simulation_run_dirs = glob.glob(simulation_dir + '/pub_simulation_*')\n",
    "for i, run_dir in enumerate(simulation_run_dirs):\n",
    "    simulation_run_dirs[i] = run_dir.split('/')[-1]\n",
    "print(f\"There are {len(simulation_run_dirs)} simulation runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 assimilation runs.\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all assimilation results.\n",
    "assimilation_run_dirs = glob.glob(assimilation_dir + '/**')\n",
    "for i, run_dir in enumerate(assimilation_run_dirs):\n",
    "    assimilation_run_dirs[i] = run_dir.split('/')[-1]\n",
    "print(f\"There are {len(assimilation_run_dirs)} assimilation runs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 10 autoregression runs.\n"
     ]
    }
   ],
   "source": [
    "# Get paths to all autoregression results.\n",
    "autoregression_run_dirs = glob.glob(autoregression_dir + '/**')\n",
    "for i, run_dir in enumerate(autoregression_run_dirs):\n",
    "    autoregression_run_dirs[i] = run_dir.split('/')[-1]\n",
    "print(f\"There are {len(autoregression_run_dirs)} autoregression runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infer Experiment Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensembles = set()\n",
    "for run in autoregression_run_dirs:\n",
    "    ensembles.add(int(run.split('seed_')[-1].split('_')[0]))\n",
    "ensembles = sorted(list(ensembles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoregression_statistics = {}\n",
    "assimilation_statistics = {}\n",
    "simulation_statistics = {}\n",
    "\n",
    "for metric in metrics_list:\n",
    "    autoregression_statistics[metric] = pd.DataFrame(index=basins, columns=ensembles, dtype=np.float64)\n",
    "    assimilation_statistics[metric] = pd.DataFrame(index=basins, columns=ensembles, dtype=np.float64)\n",
    "    simulation_statistics[metric] = pd.DataFrame(index=basins, columns=ensembles, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Obs Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the saved metrics. \n",
    "# Even if we just calcuated them, might as well make sure the file works.\n",
    "# You need to change this path manually because of the timestamp. It must\n",
    "# point to a time_split directory and not to a pub directory, in order to \n",
    "# have all of the basins.\n",
    "with open('runs/time_split/simulation/simulation_seed_0_1304_224615/test/model_epoch030/test_results.p', 'rb') as f:\n",
    "    data_xr = pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bf85c6363de48e192277093f4e316cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f7aa4978e48432c81090eb436256bf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa01adb4c77a40168161e6d672b7f111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c155f50f573246ffbf5fdcca0e9701de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae1d11b088c4b88b07e065ded0ae27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06766d063a004ea0a4cf9c2e5bd8e2c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9e095aae32540a5bcca66e44ce85a00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f671f0d4d9247cf9f11128b20ae24f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "691be35b354f484b8cf2af730a2ec814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acb64910410411398add8bad6a7e23b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CALCULATE_NEW_METRICS:\n",
    "    for run in simulation_run_dirs:\n",
    "        ensemble = int(run.split('seed_')[-1].split('_')[0])\n",
    "\n",
    "        with open(simulation_dir + '/' + run + '/test/model_epoch030/test_results.p', 'rb') as f:\n",
    "            run_data = pkl.load(f)\n",
    "\n",
    "        for basin in tqdm(run_data):\n",
    "\n",
    "            sim = run_data[basin]['1D']['xr'].stack(datetime=['date', 'time_step'])['QObs(mm/d)_sim']\n",
    "            sim['datetime'] = sim.coords['date']# + sim.coords['time_step']\n",
    "            obs = data_xr[basin]['1D']['xr']['QObs(mm/d)_obs'].stack(datetime=['date', 'time_step'])\n",
    "            obs['datetime'] = obs.coords['date']# + obs.coords['time_step']\n",
    "            basin_metrics = metrics.calculate_metrics(obs=obs, sim=sim, metrics=metrics_list)\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                simulation_statistics[metric].loc[basin, ensemble] = basin_metrics[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.702759\n",
       "dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_statistics['NSE'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assimilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e408828d7a454cb22ef33856efcffa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a6e5ddb95e4e979bb8f1e14d2bf742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_64295/2018611400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_xr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbasin\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'1D'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'xr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'QObs(mm/d)_obs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'time_step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'datetime'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m# + obs.coords['time_step']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mbasin_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lstm_data_assimilation/neuralhydrology/evaluation/metrics.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(obs, sim, metrics, resolution, datetime_coord)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"FLV\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfdc_flv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"peak-timing\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Peak-Timing\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmean_peak_timing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresolution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresolution\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime_coord\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdatetime_coord\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Unknown metric {metric}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/lstm_data_assimilation/neuralhydrology/evaluation/metrics.py\u001b[0m in \u001b[0;36mmean_peak_timing\u001b[0;34m(obs, sim, window, resolution, datetime_coord)\u001b[0m\n\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m         \u001b[0;31m# check if the value at idx is a peak (both neighbors must be smaller)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m             \u001b[0mpeak_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/_typed_ops.py\u001b[0m in \u001b[0;36m__gt__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__gt__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_binary_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__ge__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/dataarray.py\u001b[0m in \u001b[0;36m_binary_op\u001b[0;34m(self, other, f, reflexive)\u001b[0m\n\u001b[1;32m   3083\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_variable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         )\n\u001b[0;32m-> 3085\u001b[0;31m         \u001b[0mcoords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoords\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_merge_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother_coords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreflexive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3086\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3087\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/coordinates.py\u001b[0m in \u001b[0;36m_merge_raw\u001b[0;34m(self, other, reflexive)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0mcoord_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreflexive\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmerge_coordinates_without_align\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoord_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_coordinates_without_align\u001b[0;34m(objects, prioritized, exclude_dims, combine_attrs)\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0mfiltered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmerge_collected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprioritized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombine_attrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombine_attrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36mmerge_collected\u001b[0;34m(grouped, prioritized, compat, combine_attrs)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mvariable\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvariable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0melements_list\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     \u001b[0mmerged_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique_variable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mMergeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcompat\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"minimal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/merge.py\u001b[0m in \u001b[0;36munique_variable\u001b[0;34m(name, variables, compat, equals)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mequals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mequals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mbroadcast_equals\u001b[0;34m(self, other, equiv)\u001b[0m\n\u001b[1;32m   1935\u001b[0m         \"\"\"\n\u001b[1;32m   1936\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1937\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbroadcast_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1938\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1939\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/neuralhydrology/lib/python3.7/site-packages/xarray/core/variable.py\u001b[0m in \u001b[0;36mbroadcast_variables\u001b[0;34m(*variables)\u001b[0m\n\u001b[1;32m   2861\u001b[0m     \"\"\"\n\u001b[1;32m   2862\u001b[0m     \u001b[0mdims_map\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unified_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2863\u001b[0;31m     \u001b[0mdims_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2864\u001b[0m     return tuple(\n\u001b[1;32m   2865\u001b[0m         \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims_map\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mvar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdims_tuple\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mvar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if CALCULATE_NEW_METRICS:\n",
    "    for run in tqdm(assimilation_run_dirs):\n",
    "        hf = float(run.split('holdout_')[-1].split('_')[0])   \n",
    "        lead_time = int(run.split('lead_')[-1].split('_')[0])\n",
    "        ensemble = int(run.split('seed_')[-1].split('_')[0])\n",
    "\n",
    "        with open(assimilation_dir + '/' + run + '/test/model_epoch030/test_results_data_assimilation.p', 'rb') as f:\n",
    "            run_data = pkl.load(f)\n",
    "\n",
    "        for basin in tqdm(run_data):\n",
    "            sim = run_data[basin]['1D']['xr'].stack(datetime=['date', 'time_step'])['QObs(mm/d)_sim']\n",
    "            sim['datetime'] = sim.coords['date']# + sim.coords['time_step']\n",
    "            obs = data_xr[basin]['1D']['xr']['QObs(mm/d)_obs'].stack(datetime=['date', 'time_step'])\n",
    "            obs['datetime'] = obs.coords['date']# + obs.coords['time_step']\n",
    "            basin_metrics = metrics.calculate_metrics(obs=obs, sim=sim, metrics=metrics_list)\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                assimilation_statistics[metric].loc[basin, ensemble] = basin_metrics[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0    0.771882\n",
       "0.5    0.771882\n",
       "1.0    0.771882\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assimilation_statistics['NSE'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoregression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9038c3192fe84cf484939f631e2f0306",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "502c50b0e95f41698b52f3523e4994b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b2f9a1bd3349da98116ca6a4436680",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/54 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2dc32a6031af4ef98297a6305c3506e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8493a025a7ef4da59d28a9e3735bdfb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6635edef5004ec4a3633bf7c96c1bec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24e528b0b8545c49fb4e792a60c189a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da8072c3132c4526a7bd1abe2d5a8f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f36d8b96550748f0b8f3d862a33c98fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a139a36e69db40319267c578036985b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c1f0dc03cce4f888b83b49a753e054a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/53 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if CALCULATE_NEW_METRICS:\n",
    "    for run in tqdm(autoregression_run_dirs):\n",
    "        ensemble = int(run.split('seed_')[-1].split('_')[0])\n",
    "\n",
    "        with open(autoregression_dir + '/' + run + '/test/model_epoch030/test_results.p', 'rb') as f:\n",
    "            run_data = pkl.load(f)\n",
    "\n",
    "        for basin in tqdm(run_data):\n",
    "            sim = run_data[basin]['1D']['xr'].stack(datetime=['date', 'time_step'])['QObs(mm/d)_sim']\n",
    "            sim['datetime'] = sim.coords['date']# + sim.coords['time_step']\n",
    "            obs = data_xr[basin]['1D']['xr']['QObs(mm/d)_obs'].stack(datetime=['date', 'time_step'])\n",
    "            obs['datetime'] = obs.coords['date']# + obs.coords['time_step']\n",
    "            basin_metrics = metrics.calculate_metrics(obs=obs, sim=sim, metrics=metrics_list)\n",
    "\n",
    "            for metric in metrics_list:\n",
    "                autoregression_statistics[metric].loc[basin, ensemble] = basin_metrics[metric]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.806685\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "autoregression_statistics['NSE'].median().dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save & Load Precaculated Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we calculated new metrics, save these over any old ones that might exist.\n",
    "if CALCULATE_NEW_METRICS:\n",
    "    with open(stats_filename, 'wb') as f:\n",
    "        pkl.dump([\n",
    "            simulation_statistics,\n",
    "            autoregression_statistics,\n",
    "            assimilation_statistics\n",
    "        ], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in the saved metrics. \n",
    "# Even if we just calcuated them, might as well make sure the file works.\n",
    "with open(time_split_stats_filename, 'rb') as f:\n",
    "    simulation_statistics, autoregression_statistics, assimilation_statistics = pkl.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assimilation_statistics['NSE'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 1\n",
    "\n",
    "Median NSE scores of AR models trained and tested with different fractions of lagged streamflow data withheld. The two subplots\n",
    "show the same results, but organized by the amount of lagged streamflow data withheld during training vs. during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By test holdout\n",
    "lead_time = 1\n",
    "metric = 'NSE'\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "for hf in test_holdout_fractions:\n",
    "    plotdata = autoregression_statistics[metric].median().loc[hf] \n",
    "    plotdata = plotdata.loc[(slice(None), lead_time, 0)]\n",
    "    axes[0].plot(test_holdout_fractions, plotdata.values, label=hf)\n",
    "axes[1].plot([0,1], [simulation_statistics[metric].values[0], simulation_statistics[metric].values[0]], 'k--', label='Sim')\n",
    "axes[0].set_xlabel('test holdout fraction')\n",
    "axes[0].legend(title=\"train holdout fraction\", prop={'size': 6})\n",
    "axes[0].set_ylabel(metric)\n",
    "axes[0].grid()\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "for hf in train_holdout_fractions:\n",
    "    plotdata = autoregression_statistics[metric].median().loc[(slice(None), hf)] \n",
    "    plotdata = plotdata.loc[(slice(None), lead_time, 0)]\n",
    "    print(plotdata.values)\n",
    "    asdf\n",
    "    axes[1].plot(test_holdout_fractions, plotdata.values, label=hf)\n",
    "axes[1].plot([0,1], [simulation_statistics[metric].values[0], simulation_statistics[metric].values[0]], 'k--', label='Sim')\n",
    "axes[1].set_xlabel('train holdout fraction')\n",
    "axes[1].legend(title=\"test holdout fraction\", prop={'size': 6})\n",
    "axes[1].set_ylabel(metric)\n",
    "axes[1].grid()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_file_name = f\"./results/plots/figure1_ar_holdout.png\"\n",
    "plt.savefig(plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 2\n",
    "\n",
    "Comparison of per-basin NSEs with an observation lag of one day and no missing streamflow input data: (left) autoregression\n",
    "trained with no holdout data vs. variational assimilation and (right) autoregression with vs. without 50% of lagged streamflow data withheld\n",
    "during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = 'NSE'\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
    "\n",
    "xdata = autoregression_statistics[metric].median().loc[(0.0, 0.0, 1, 0)]\n",
    "ydata = assimilation_statistics[metric][(0.0, 1, 0)]\n",
    "axes[0].scatter(xdata, ydata)\n",
    "axes[0].plot([0, 1], [0, 1], 'k--')\n",
    "axes[0].set_xlabel('AR 0.0 Holdout')\n",
    "axes[0].set_ylabel('Variational Assimilation')\n",
    "axes[0].set_title('NSE (1-day Lag, 0% Missing Data)')\n",
    "axes[0].grid()\n",
    "\n",
    "xdata = autoregression_statistics[metric].median().loc[(0.0, 0.0, 1, 0)]\n",
    "ydata = autoregression_statistics[metric].median().loc[(0.5, 0.0, 1, 0)]\n",
    "axes[1].scatter(xdata, ydata)\n",
    "axes[1].plot([0, 1], [0, 1], 'k--')\n",
    "axes[1].set_xlabel('AR 0.0 Holdout')\n",
    "axes[1].set_ylabel('AR 0.5 Holdout')\n",
    "axes[1].set_title('NSE (1-day Lag, 0% Missing Test Data)')\n",
    "axes[1].grid()\n",
    "\n",
    "plot_file_name = f\"./plots/figure2_scatterplot.png\"\n",
    "plt.savefig(plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figure 3 & Appendix G\n",
    "\n",
    "Median NSE over 531 basins of four models (simulation, autoregression trained with and without holdout data, and data assimilation) as a function of lag time in days and fraction of missing lagged streamflow data in the test period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in metrics_list:\n",
    "\n",
    "    ar_plotdata_hf = pd.DataFrame(index=lead_times, columns=train_holdout_fractions)\n",
    "    for lt in lead_times:\n",
    "        for hf in train_holdout_fractions:\n",
    "            ar_plotdata_hf.loc[lt, hf] = autoregression_statistics[metric].median().loc[(hf, hf, lt, 0)]\n",
    "\n",
    "    ar_plotdata_0 = pd.DataFrame(index=lead_times, columns=train_holdout_fractions)\n",
    "    for lt in lead_times:\n",
    "        for hf in train_holdout_fractions:\n",
    "            ar_plotdata_0.loc[lt, hf] = autoregression_statistics[metric].median().loc[(0.0, hf, lt, 0)]\n",
    "\n",
    "    da_plotdata = pd.DataFrame(index=lead_times, columns=train_holdout_fractions)\n",
    "    for lt in lead_times:\n",
    "        for hf in train_holdout_fractions:\n",
    "            da_plotdata.loc[lt, hf] = assimilation_statistics[metric].median().loc[(hf, lt, 0)]\n",
    "    \n",
    "    ymin = np.min([simulation_statistics[metric].values[0], ar_plotdata_hf.min().min(), ar_plotdata_0.min().min(), da_plotdata.min().min()]) * 0.99\n",
    "    ymax = np.max([simulation_statistics[metric].values[0], ar_plotdata_hf.max().max(), ar_plotdata_0.max().max(), da_plotdata.max().max()]) * 1.01\n",
    "\n",
    "    fig, axes = plt.subplots(5, 1, figsize=(8, 7))\n",
    "    for i, lead_time in enumerate(lead_times):\n",
    "        axes.flatten()[i].plot(simulation_statistics[metric].median(), label='Sim')\n",
    "        axes.flatten()[i].plot(ar_plotdata_hf.loc[lead_time], label='AR w/ holdout')\n",
    "        axes.flatten()[i].plot(ar_plotdata_0.loc[lead_time], label='AR w/o holdout')\n",
    "        axes.flatten()[i].plot(da_plotdata.loc[lead_time], label='DA')\n",
    "        if i == 5: axes.flatten()[i].legend()\n",
    "        axes.flatten()[i].set_xlabel('fraction of missing data')\n",
    "        axes.flatten()[i].set_ylim([ymin, ymax])\n",
    "        axes.flatten()[i].set_title(f'{lead_time} Days')\n",
    "        \n",
    "    plot_file_name = f\"./plots/{metric}_lead_times_plots.png\"\n",
    "    plt.savefig(plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table 2\n",
    "\n",
    "Median NSE scores of AR models trained and tested with different fractions of lagged streamflow data withheld. The two subplots\n",
    "show the same results, but organized by the amount of lagged streamflow data withheld during training vs. during testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Allow this to work with ensembles instead of just pulling the 0th member.\n",
    "models_list = ['Simulation', 'AR 0.0 holdout', 'AR 0.5 holdout', 'Assimilation']\n",
    "table_df = pd.DataFrame(index=metrics_list, columns=models_list)\n",
    "for metric in metrics_list:\n",
    "    table_df.loc[metric, 'Simulation'] = simulation_statistics[metric].median().values[0]\n",
    "    table_df.loc[metric, 'AR 0.0 holdout'] = autoregression_statistics[metric].median().loc[(0.0, 0.0, 1, 0)]\n",
    "    table_df.loc[metric, 'AR 0.5 holdout'] = autoregression_statistics[metric].median().loc[(0.5, 0.0, 1, 0)]\n",
    "    table_df.loc[metric, 'Assimilation'] = assimilation_statistics[metric].median().loc[(0.0, 1, 0)]\n",
    "table_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to file in latex format.\n",
    "table_filename = 'results/tables/zero_lag_zero_holdout_metrics_table.txt''\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "with open(table_filename, 'wt') as f:\n",
    "    f.write(table_df.to_latex())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
